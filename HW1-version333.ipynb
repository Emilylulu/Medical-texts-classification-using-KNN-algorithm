{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from numpy.linalg import norm\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "from nltk.corpus import stopwords\n",
    "import ipywidgets as widgets\n",
    "\n",
    "idx={}\n",
    "mat=0\n",
    "doc=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_matrix(docs):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    nrows = len(docs)\n",
    "    global idx\n",
    "    \n",
    "    tid = 0\n",
    "    nnz = 0\n",
    "    for d in docs:\n",
    "        nnz += len(set(d))\n",
    "        for w in d:\n",
    "            if w not in idx:\n",
    "                idx[w] = tid\n",
    "                tid += 1\n",
    "    ncols = len(idx)\n",
    "        \n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in docs:\n",
    "        cnt = Counter(d)\n",
    "        keys = list(k for k,_ in cnt.most_common())\n",
    "        l = len(keys)\n",
    "        for j,k in enumerate(keys):\n",
    "            ind[j+n] = idx[k]\n",
    "            val[j+n] = cnt[k]\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "            \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildMatrix(docs,test):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    nrows = len(test)\n",
    "    idx = {}\n",
    "    tid = 0\n",
    "    nnz = 0\n",
    "    for d in docs:\n",
    "        nnz += len(set(d))\n",
    "        for w in d:\n",
    "            if w not in idx:\n",
    "                idx[w] = tid\n",
    "                tid += 1\n",
    "    ncols = len(idx)\n",
    "        \n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in test:\n",
    "        cnt = Counter(d)\n",
    "        keys = list(k for k,_ in cnt.most_common())\n",
    "        l = len(keys)\n",
    "        for j,k in enumerate(keys):\n",
    "            if cnt[k] not in idx:\n",
    "                continue\n",
    "            ind[j+n] = idx[k]\n",
    "            val[j+n] = cnt[k]\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "            \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cmer(text,cmer=3,has_cut=True):\n",
    "    r\"\"\" Given a text and parameter c, return the vector of c-mers associated with the text\n",
    "    \"\"\"\n",
    "    temp=text\n",
    "    if not has_cut:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        for t in text:\n",
    "            t=t.lower()\n",
    "            x=t.split()\n",
    "    #     temp=x\n",
    "        temp=[]\n",
    "        for i in x:\n",
    "            if i not in stop_words:\n",
    "                i=st1.stem(i)\n",
    "                temp.append(i)\n",
    "        text=temp\n",
    "    v=[]\n",
    "    textLength=len(text)\n",
    "    p = 0\n",
    "    while(p + cmer <= textLength):\n",
    "        s=\"\"\n",
    "        for i in range(cmer):\n",
    "            s+=text[p+i]\n",
    "        v.append(s)\n",
    "        p += 1\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def csr_info(mat, name=\"\", non_empy=False):\n",
    "    r\"\"\" Print out info about this CSR matrix. If non_empy, \n",
    "    report number of non-empty rows and cols as well\n",
    "    \"\"\"\n",
    "    if non_empy:\n",
    "        print(\"%s [nrows %d (%d non-empty), ncols %d (%d non-empty), nnz %d]\" % (\n",
    "                name, mat.shape[0], \n",
    "                sum(1 if mat.indptr[i+1] > mat.indptr[i] else 0 \n",
    "                for i in range(mat.shape[0])), \n",
    "                mat.shape[1], len(np.unique(mat.indices)), \n",
    "                len(mat.data)))\n",
    "    else:\n",
    "        print( \"%s [nrows %d, ncols %d, nnz %d]\" % (name, \n",
    "                mat.shape[0], mat.shape[1], len(mat.data)) )\n",
    "        \n",
    "def textsToMatrix(doc, c):\n",
    "    docs = [cmer(t, c) for t in doc]\n",
    "    return build_matrix(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d7d888f1a1f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnewdoc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mst1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnewdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "newdoc=[]\n",
    "for word in doc[0]:\n",
    "    word=st1.stem(word)\n",
    "    newdoc.append(word)\n",
    "doc[0]=newdoc\n",
    "print(doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  app.launch_new_instance()\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "st1 = LancasterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# read test data and class info\n",
    "df = pd.read_table('./train.dat',sep='\\t',header=None)\n",
    "texts = df.ix[:,:].values\n",
    "\n",
    "# doc=pd.read_table('./format.dat',sep='\\t',header=None)\n",
    "# docs=doc.ix[:,:].values\n",
    "ori_docs=[]\n",
    "cls=[]\n",
    "for i in texts:\n",
    "    cls.append(i[0])\n",
    "    ori_docs.append(i[1:])\n",
    "test = pd.read_table('./test.dat',sep='\\t',header=None)\n",
    "test = test.ix[::].values\n",
    "#   upload data for test label\n",
    "t_cls = pd.read_table('./format.dat',sep='\\t',header=None)\n",
    "t_cls = t_cls.ix[::].values\n",
    "test_cls = []\n",
    "global doc\n",
    "doc=[]\n",
    "for i in t_cls:\n",
    "    test_cls.append(i[0])\n",
    "for d in ori_docs:\n",
    "    for t in d:\n",
    "        x=t.split()    \n",
    "    result=[]\n",
    "    for i in x:\n",
    "        if i not in stop_words:\n",
    "            i=i.lower()\n",
    "# stem words to small data\n",
    "            i=st1.stem(i)\n",
    "            result.append(i)\n",
    "    doc.append(result)\n",
    "# testset=[]\n",
    "# for te in test:\n",
    "#     for se in te:\n",
    "#         p=se.split()\n",
    "#     re=[]\n",
    "#     for j in p:\n",
    "#         if j not in stop_words:\n",
    "#             re.append(j.lower())\n",
    "#     testset.append(re)\n",
    "\n",
    "\n",
    "# mattest=textsToMatrix(testset,c=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def csr_l2normalize(mat, copy=False, **kargs):\n",
    "    r\"\"\" Normalize the rows of a CSR matrix by their L-2 norm. \n",
    "    If copy is True, returns a copy of the normalized matrix.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = 1.0/np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "            \n",
    "    if copy is True:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Expect a single post cmer processed test input array. Like [\"atest\", \"testcase\"]\n",
    "def build_test_input_csr_matrix(test):\n",
    "    size=len(set(test))\n",
    "    ind = np.zeros(size, dtype=np.int)\n",
    "    val = np.zeros(size, dtype=np.double)\n",
    "    ptr = np.zeros(2, dtype=np.int)\n",
    "    \n",
    "    cnt = Counter(test)\n",
    "    keys = list(k for k,_ in cnt.most_common())\n",
    "    l = len(keys)\n",
    "    for j,k in enumerate(keys):\n",
    "        if k not in idx:\n",
    "            continue\n",
    "        ind[j] = idx[k]\n",
    "        val[j] = cnt[k]\n",
    "    ptr[1] = ptr[0] + l\n",
    "    \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(1, len(idx)), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findNeighbors(test,doc,k=1,c=1):\n",
    "#   find test in doc and use index in mat\n",
    "    id=-1\n",
    "    for i in range(len(doc)):\n",
    "        if(np.array_equal(test,doc[i])):\n",
    "            id=i\n",
    "            break\n",
    "#     mat=textsToMatrix(doc,c)\n",
    "#   control if test is not in training set\n",
    "    \n",
    "    if(id==-1):\n",
    "        test=cmer(test,c,has_cut=False)\n",
    "        test_matrix=build_test_input_csr_matrix(test)\n",
    "        csr_l2normalize(test_matrix)\n",
    "        x=test_matrix\n",
    "#         for k in range(len(testset)):\n",
    "#             if(np.array_equal(test,testset[i])):\n",
    "#                 id = k\n",
    "#                 test=mattest[id]\n",
    "    else:\n",
    "        x = mat[id]\n",
    "#   claculate cosine similarity\n",
    "    csr_l2normalize(mat)\n",
    "    dots = x.dot(mat.T)\n",
    "    if id != -1:\n",
    "        dots[0,id] = 1 # invalidate self-similarity\n",
    "    sims = list(zip(dots.indices, dots.data))\n",
    "    sims.sort(key=lambda x: x[1], reverse=True)\n",
    "#     print(sims)\n",
    "    return [[s[0],s[1]]for s in sims[:k] if s[1] > 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifyclass(test,training=doc,cls=cls,k=10,c=3):\n",
    "    pred=[0.0,0.0,0.0,0.0,0.0]\n",
    "    neighbor=findNeighbors(test,training,k,c)\n",
    "    \n",
    "#   find neighbors class\n",
    "    for i in range(len(neighbor)):\n",
    "        label=cls[neighbor[i][0]]-1\n",
    "#         print(neighbor[i],cls[neighbor[i]])\n",
    "        pred[label] = pred[label] + neighbor[i][1]\n",
    "    maxid=0\n",
    "    for i in range(5):\n",
    "        if(pred[maxid] < pred[i]):\n",
    "            maxid=i\n",
    "#   use majority vote rule to predict label \n",
    "    \n",
    "    return maxid+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global mat\n",
    "mat=textsToMatrix(doc,c=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn=classifyclass(test[:8],doc,cls,k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csr_info(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifyclass(doc[1],doc,cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for C in range(1,7):\n",
    "    global mat\n",
    "    mat=textsToMatrix(doc,c=C)\n",
    "    y_p=[]\n",
    "    progress=widgets.IntProgress(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=len(test),\n",
    "        step=1,\n",
    "        description='cmer c=' + str(C),\n",
    "        bar_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "        orientation='horizontal'\n",
    "    )\n",
    "    display(progress)\n",
    "    for text1 in test:\n",
    "        result=classifyclass(text1,doc,cls,c=C)\n",
    "    #     print(result)\n",
    "        y_p.append(result)\n",
    "        progress.value+=1\n",
    "    count=0\n",
    "#     print(y_p)\n",
    "#     for i in range(len(y_p)):\n",
    "#         if(y_p[i]==cls[i]):\n",
    "#             count +=1\n",
    "#     print(\"c=\" + str(C) + \", accuracy:\" + str(count/len(y_p)))\n",
    "    with open('test_c_'+str(C)+'.txt', 'w') as f:\n",
    "        for item in y_p:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "        print('test_c_'+str(C)+'.txt write sucessful!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 1, 5, 2, 1, 4, 1, 1, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "y_pred=[]\n",
    "for text in test[:10]:\n",
    "    y_pred.append(classifyclass(text,doc,cls,10,3))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(idx['laboratory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "for i in range(len(y_pred)):\n",
    "    if(y_pred[i]==cls[i]):\n",
    "        count += 1\n",
    "print(count/len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newdoc=[]\n",
    "for word in doc[0]:\n",
    "    word=st1.stem(word)\n",
    "    newdoc.append(word)\n",
    "doc[0]=newdoc\n",
    "print(doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print((mat.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# from textblob import TextBlob as tb\n",
    "\n",
    "def tf(word, text):\n",
    "    return text.count(word) / len(text)\n",
    "\n",
    "# def n_containing(word, bloblist):\n",
    "#     return sum(1 for blob in bloblist if word in blob.words)\n",
    "\n",
    "def idf(word, texts):\n",
    "    count=0\n",
    "    length=0\n",
    "    for d in doc:\n",
    "        length += len(d)\n",
    "        count += d.count(word)\n",
    "    return math.log(length) / (1 + count)\n",
    "\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bloblist = doc\n",
    "deletewords=[]\n",
    "for i, blob in enumerate(bloblist):\n",
    "    print(\"need to delete words in document {}\".format(i + 1))\n",
    "    scores = {word: tfidf(word, blob, bloblist) for word in blob}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=False)\n",
    "    for word,score in sorted_words:\n",
    "        if(round(score,5)<0.00001):\n",
    "            deletewords.append(word)\n",
    "            print(deletewords)\n",
    "\n",
    "#     for word, score in sorted_words[:3]:\n",
    "#     print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word=doc[0][1]\n",
    "count=0\n",
    "length=0\n",
    "for d in doc:\n",
    "    length += len(d)\n",
    "    count += d.count(word)\n",
    "print(length,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word in doc[0]:\n",
    "    print(doc[0].count(word),word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
